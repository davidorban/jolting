---

layout: post

title: "Jolting AI"

date: 2020-02-01

---

<div class="post-header">

<h1>{{ page.title }}</h1>

<p>{{ page.date | date: "%B %d, %Y" }}</p>

</div>

Between 2012 and 2018 the power of the infrastructure available for applications in artificial intelligence increased over three hundred thousand times. If it followed Mooreâ€™s law, it would have been much less, around 7 times or so.

<iframe width="560" height="315" src="https://www.youtube.com/embed/ajoyra88UkA" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

Stanford University in its [2019 report on AI](https://hai.stanford.edu/ai-index/2019), and in a [blog post OpenAI](https://openai.com/blog/ai-and-compute/) analyzed and illustrated this recently.

![Jolting AI Chart 1](assets/images/jolting-ai-2.jpg)

We can apply a simple linear interpolation and view two segments, one where computing power doubles every two years, and one where it doubles every three-four months.

![Jolting AI Chart 2](assets/images/jolting-ai-3.jpg)

But it is much more useful to think in terms of increasing acceleration of computing power.

![Jolting AI Chart 3](assets/images/jolting-ai-4.jpg)

This better prepares us to face the future changes, additional increases in acceleration, when quantum computers are going to be used by AI systems to design more powerful versions of themselves. Will the doubling rate become a few weeks, a few hours?
